{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torchvision.datasets import FashionMNIST\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nbatch_size = 256\ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# Data preparation for the test dataset\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T18:05:52.448043Z","iopub.execute_input":"2024-12-08T18:05:52.448328Z","iopub.status.idle":"2024-12-08T18:06:03.962638Z","shell.execute_reply.started":"2024-12-08T18:05:52.448304Z","shell.execute_reply":"2024-12-08T18:06:03.961736Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\n\ndef print_dataset_image_dimensions(dataset):\n    \"\"\"Prints the dimensions of an image in a PyTorch dataset.\"\"\"\n    try:\n        \n        sample = dataset[0]\n        if isinstance(sample, tuple):\n            image = sample[0]\n        elif hasattr(sample, 'shape'):\n            image = sample\n        else:\n            print(\"Error: Dataset sample does not have an image or shape attribute.\")\n            return\n        \n        print(f\"Image shape: {image.shape}\")\n    except IndexError as e:\n        print(f\"Error accessing dataset element: {e}\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n\n\nprint_dataset_image_dimensions(train_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T18:06:45.305436Z","iopub.execute_input":"2024-12-08T18:06:45.305802Z","iopub.status.idle":"2024-12-08T18:06:45.312661Z","shell.execute_reply.started":"2024-12-08T18:06:45.305771Z","shell.execute_reply":"2024-12-08T18:06:45.311825Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(train_dataset))\nprint(len(test_dataset))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T18:06:47.531575Z","iopub.execute_input":"2024-12-08T18:06:47.531946Z","iopub.status.idle":"2024-12-08T18:06:47.536566Z","shell.execute_reply.started":"2024-12-08T18:06:47.531909Z","shell.execute_reply":"2024-12-08T18:06:47.535709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\n\n\n# Get the first batch of images and labels\ndata_iter = iter(train_loader)\nimages, labels = next(data_iter)\n\n\nimage = images[13] \nlabel = labels[0]  \n\nplt.figure(figsize=(1, 1))  \nplt.imshow(image.squeeze(), cmap=\"gray\")\nplt.title(f\"Label: {label}\")\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T18:06:49.844050Z","iopub.execute_input":"2024-12-08T18:06:49.844851Z","iopub.status.idle":"2024-12-08T18:06:50.029973Z","shell.execute_reply.started":"2024-12-08T18:06:49.844816Z","shell.execute_reply":"2024-12-08T18:06:50.028764Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Encoder and decoder of VAE are decoded as follows.**\n- Encoder (Gaussian distribution)：$$q_{\\phi}({\\bf z}|{\\bf x}) = {\\mathcal N}({\\bf z}| \\mu,\\sigma^2{\\bf I}), 　s.t.　\\mu=g^{\\mu}_{\\phi}({\\bf x}), \\sigma=g^{\\sigma}_{\\phi}({\\bf x}). $$\n- Decoder (Bernoulli distribution)：$$p_{\\theta}({\\bf x}|{\\bf z}) = Ber({\\bf x}| \\lambda), 　s.t.　\\lambda=f_{\\theta}({\\bf z}).$$\n<br>\n\n**ELBO (Evidence Lower BOund) is represented like below.**  The first and the second term are corresponding to (minus) reconstruction loss and KL divergence, respectively. For the implementation of β-VAE, the coefficient β(>1) in the regularisation term of the objective function is needed in order to make the model more disentangled.\n\n\n$$\n {\\mathcal L}({\\bf x};{\\bf \\theta},{\\bf \\phi}) = \\mathbb{E}_{q_{\\phi}({\\bf z}|{\\bf x})}[\\log p_\\theta({\\bf x}|{\\bf z})] -\\beta D_{KL}[q_{\\phi}({\\bf z}|{\\bf x})||p_{\\theta}({\\bf z})]\n $$  ","metadata":{}},{"cell_type":"markdown","source":"#**MODEL**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BetaVAE(nn.Module):\n    def __init__(self, latent_dim=10, beta=1.0):\n        super(BetaVAE, self).__init__()\n        self.latent_dim = latent_dim\n        self.beta = beta\n\n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 8, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=0),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(32 * 3 * 3, 128),\n        )\n\n        self.fc_mu = nn.Linear(128, latent_dim)\n        self.fc_logvar = nn.Linear(128, latent_dim)\n\n        # Decoder\n        self.decoder_fc = nn.Sequential(\n            nn.Linear(latent_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 32 * 3 * 3),\n            nn.ReLU(),\n        )\n\n        self.decoder_conv = nn.Sequential(\n            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=0, output_padding=0),  # From 3x3 to 7x7\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=1, output_padding=1),  # From 7x7 to 14x14\n            nn.ReLU(),\n            nn.ConvTranspose2d(8, 1, kernel_size=4, stride=2, padding=1, output_padding=0),  # From 14x14 to 28x28\n            nn.Sigmoid(),\n        )\n\n\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        # Encoding\n        x = self.encoder(x)\n        mu = self.fc_mu(x)\n        logvar = self.fc_logvar(x)\n\n        # Reparameterization\n        z = self.reparameterize(mu, logvar)\n\n        # Decoding\n        x = self.decoder_fc(z)\n        x = x.view(-1, 32, 3, 3)\n        x = self.decoder_conv(x)\n        return x, mu, logvar\n\n\n    def decode(self, z):\n        \"\"\"Decode a latent vector `z` into an image.\"\"\"\n        x = self.decoder_fc(z)\n        x = x.view(-1, 32, 3, 3)\n        x = self.decoder_conv(x)\n        return x\n\n    \n    def loss_function(self, recon_x, x, mu, logvar):\n        recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n        return recon_loss + self.beta * kl_loss\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T18:07:02.986822Z","iopub.execute_input":"2024-12-08T18:07:02.987151Z","iopub.status.idle":"2024-12-08T18:07:03.000598Z","shell.execute_reply.started":"2024-12-08T18:07:02.987120Z","shell.execute_reply":"2024-12-08T18:07:02.999651Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loss Function","metadata":{}},{"cell_type":"code","source":"def loss_function(self, recon_x, x, mu, logvar):\n        recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n        return recon_loss + self.beta * kl_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T18:07:07.644592Z","iopub.execute_input":"2024-12-08T18:07:07.645203Z","iopub.status.idle":"2024-12-08T18:07:07.649608Z","shell.execute_reply.started":"2024-12-08T18:07:07.645168Z","shell.execute_reply":"2024-12-08T18:07:07.648782Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **TRAINING**","metadata":{}},{"cell_type":"code","source":"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T18:07:10.464602Z","iopub.execute_input":"2024-12-08T18:07:10.464982Z","iopub.status.idle":"2024-12-08T18:07:10.556343Z","shell.execute_reply.started":"2024-12-08T18:07:10.464951Z","shell.execute_reply":"2024-12-08T18:07:10.555445Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nlatent_dim = 10\nbeta = 0.5  # Adjust based on experiments for balance between reconstruction and disentanglement\nepochs = 20\nlearning_rate = 1e-3\n\n\nmodel = BetaVAE(latent_dim=latent_dim, beta=beta).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate , weight_decay=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T18:07:13.443505Z","iopub.execute_input":"2024-12-08T18:07:13.444332Z","iopub.status.idle":"2024-12-08T18:07:13.649004Z","shell.execute_reply.started":"2024-12-08T18:07:13.444296Z","shell.execute_reply":"2024-12-08T18:07:13.648047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(epochs):\n    model.train()\n    train_loss = 0\n    for batch in train_loader:  # Assuming train_loader is defined\n        images, _ = batch\n        images = images.to(device)\n\n        optimizer.zero_grad()\n        recon_images, mu, logvar = model(images)\n        loss = model.loss_function(recon_images, images, mu, logvar)\n        loss.backward()\n        train_loss += loss.item()\n        optimizer.step()\n\n    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {train_loss / len(train_loader.dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T18:07:17.764636Z","iopub.execute_input":"2024-12-08T18:07:17.765447Z","iopub.status.idle":"2024-12-08T18:09:25.492020Z","shell.execute_reply.started":"2024-12-08T18:07:17.765413Z","shell.execute_reply":"2024-12-08T18:09:25.491269Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Visualization of reconstructed image**","metadata":{}},{"cell_type":"markdown","source":"Original image","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_reconstructions_and_generations(model, test_loader, num_examples=100):\n    import matplotlib.pyplot as plt\n    model.eval()\n    \n    images_collected = []\n    reconstructions_collected = []\n    \n    with torch.no_grad():\n        for images, _ in test_loader:\n            images = images.to(device)\n            recon_images, _, _ = model(images)\n            images_collected.append(images)\n            reconstructions_collected.append(recon_images)\n            \n            # Stop when we collect enough examples\n            if len(images_collected) * batch_size >= num_examples:\n                break\n    \n    images_collected = torch.cat(images_collected, dim=0)[:num_examples]\n    reconstructions_collected = torch.cat(reconstructions_collected, dim=0)[:num_examples]\n    \n    fig, axes = plt.subplots(2, 100, figsize=(num_examples, 10))\n    for i in range(100):  # Show 10 examples\n        axes[0, i].imshow(images_collected[i].cpu().squeeze(), cmap=\"gray\")\n        axes[0, i].axis(\"off\")\n        axes[1, i].imshow(reconstructions_collected[i].cpu().squeeze(), cmap=\"gray\")\n        axes[1, i].axis(\"off\")\n    axes[0, 0].set_title(\"Originals\", fontsize=16)\n    axes[1, 0].set_title(\"Reconstructions\", fontsize=16)\n    plt.show()\n\n    noise = torch.randn(num_examples, 10).to(device)  # Latent dimension = 10\n    with torch.no_grad():\n        generated_images = model.decode(noise)\n    \n    fig, axes = plt.subplots(1, 100, figsize=(num_examples, 10))\n    for i in range(100):  # Show 10 generated examples\n        axes[i].imshow(generated_images[i].cpu().squeeze(), cmap=\"gray\")\n        axes[i].axis(\"off\")\n    plt.suptitle(\"Generated Examples\", fontsize=16)\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T18:09:39.678786Z","iopub.execute_input":"2024-12-08T18:09:39.679130Z","iopub.status.idle":"2024-12-08T18:09:39.687986Z","shell.execute_reply.started":"2024-12-08T18:09:39.679100Z","shell.execute_reply":"2024-12-08T18:09:39.687147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_reconstructions_and_generations(model, test_loader, num_examples=100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T18:09:43.043074Z","iopub.execute_input":"2024-12-08T18:09:43.043408Z","iopub.status.idle":"2024-12-08T18:09:50.395297Z","shell.execute_reply.started":"2024-12-08T18:09:43.043379Z","shell.execute_reply":"2024-12-08T18:09:50.394488Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Reconstructed image","metadata":{}},{"cell_type":"markdown","source":"Random sampling from latent variable","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.models import inception_v3\nfrom scipy.linalg import sqrtm\nimport numpy as np\n\n\n\ndef calculate_fid(real_features, generated_features):\n    mu_real = np.mean(real_features, axis=0)\n    sigma_real = np.cov(real_features, rowvar=False)\n    mu_gen = np.mean(generated_features, axis=0)\n    sigma_gen = np.cov(generated_features, rowvar=False)\n\n    diff = mu_real - mu_gen\n    covmean, _ = sqrtm(sigma_real.dot(sigma_gen), disp=False)\n\n    if np.iscomplexobj(covmean):\n        covmean = covmean.real\n\n    fid = diff.dot(diff) + np.trace(sigma_real + sigma_gen - 2 * covmean)\n    return fid\n\ninception = inception_v3(pretrained=True, transform_input=False).to(device)\ninception.fc = nn.Identity()\nreal_features = []\ngen_features = []\n\nwith torch.no_grad():\n    for images, _ in test_loader:\n        images = images.to(device)\n\n        # Resize images to 75x75 and convert to RGB\n        images_resized = F.interpolate(images, size=(75, 75), mode='bilinear')\n        images_rgb = images_resized.repeat(1, 3, 1, 1)\n\n        real_features.append(inception(images_rgb).cpu().numpy())\n\n    for gen_images in generated_images:\n        gen_images_resized = F.interpolate(gen_images, size=(75, 75), mode='bilinear')\n        gen_images_rgb = gen_images_resized.repeat(1, 3, 1, 1)\n\n        gen_features.append(inception(gen_images_rgb).cpu().numpy())\n\nreal_features = np.concatenate(real_features, axis=0)\ngen_features = np.concatenate(gen_features, axis=0)\n\nfid_score = calculate_fid(real_features, gen_features)\nprint(f\"FID Score: {fid_score}\")\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T18:34:17.427215Z","iopub.execute_input":"2024-12-08T18:34:17.427869Z","iopub.status.idle":"2024-12-08T18:34:17.922329Z","shell.execute_reply.started":"2024-12-08T18:34:17.427834Z","shell.execute_reply":"2024-12-08T18:34:17.920957Z"}},"outputs":[],"execution_count":null}]}